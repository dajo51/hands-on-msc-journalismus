{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb0ea52",
   "metadata": {},
   "source": [
    "# Was ist und kann Transformers?\n",
    "Kurze Einführung und Vorstellung der Funktionen des transformers python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc54bc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.5.1)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venv/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venv/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: torch==2.5.1 in ./venv/lib/python3.12/site-packages (from torchaudio) (2.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch==2.5.1->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch==2.5.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch==2.5.1->torchaudio) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch==2.5.1->torchaudio) (75.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.12/site-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installation des Pakets (Nicht notwendig bei Google Colab)\n",
    "!pip install pandas transformers torchaudio pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c182a",
   "metadata": {},
   "source": [
    "## Transkription von Audiodateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd74f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/Users/davidjosiger/Desktop/dev/hands-on-msc-journalismus/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' für unser Land zu treffen. Darum ging es mir in den vergangenen drei Jahren. Darum geht es mir jetzt. Ich habe dem Koalitionspartner von der FDP heute Mittag noch einmal einen Umfang'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Neue Pipeline erstellen und Aufgabe zuweisen\n",
    "\n",
    "# transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
    "\n",
    "# transcriber(\"test.mp3\")\n",
    "\n",
    "transcriber(\"test.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959afb88",
   "metadata": {},
   "source": [
    "## Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cae6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.17.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.10-cp312-cp312-macosx_11_0_arm64.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.9/390.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.0.0-cp312-cp312-macosx_12_0_arm64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading yarl-1.17.1-cp312-cp312-macosx_11_0_arm64.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 attrs-24.2.0 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-18.0.0 xxhash-3.5.0 yarl-1.17.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "855636d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 455748.05 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 700400.10 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 667398.19 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Datensätze laden und untersuchen\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "imdb[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91d487",
   "metadata": {},
   "source": [
    "# Analyse von Nachrichtenartikeln\n",
    "In diesem Abschnitt verwenden wir verschiedene NLP-Tools von Hugging Face, um einen großen Datensatz voller Spiegel-Artikel zu analysieren. Mit Hilfe von Transformer-Modellen führen wir Sentiment-Analyse, Named Entity Recognition (NER) und Topic Modeling durch, um wichtige Einblicke in die Artikel zu gewinnen.\n",
    "\n",
    "### Ziel:\n",
    "- Laden und Verarbeiten großer JSON-Datensätze in Chunks\n",
    "- Durchführung von Sentiment-Analyse und NER auf Artikeldaten\n",
    "- Visualisierung und Analyse der Ergebnisse\n",
    "\n",
    "**Benötigte Zeit:** ~2 Stunden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d576ce5",
   "metadata": {},
   "source": [
    "## 1. Setup und Laden der JSON-Daten in Chunks\n",
    "Wir beginnen mit der Installation der notwendigen Bibliotheken und laden die JSON-Daten in verwaltbaren Chunks, damit wir große Dateien effizient verarbeiten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# JSON-Daten in Chunks laden\n",
    "def load_json_from_github_in_chunks(url, chunk_size=1000):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    articles = list(data.values())\n",
    "    for i in range(0, len(articles), chunk_size):\n",
    "        yield articles[i:i + chunk_size]\n",
    "\n",
    "# Pfad zur Datei angeben (Pfad entsprechend in Google Colab anpassen oder direkt von URL ziehen)\n",
    "github_url = 'https://raw.githubusercontent.com/dajo51/hands-on-msc-journalismus/main/data_2009.json'\n",
    "\n",
    "# Beispiel-Chunks laden und Struktur inspizieren\n",
    "sample_chunk = next(load_json_from_github_in_chunks(github_url))\n",
    "df_sample = pd.DataFrame(sample_chunk)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5ebf",
   "metadata": {},
   "source": [
    "## 2. Extrahieren und Verarbeiten der Schlüsselinformationen\n",
    "Wir transformieren die JSON-Daten in ein DataFrame-Format, um sie leichter analysieren zu können. Hier extrahieren wir Felder wie `title`, `description`, `text` und `keywords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ea988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funktion zum Extrahieren der Schlüssel-Felder\n",
    "def extract_fields(article_chunk):\n",
    "    data = []\n",
    "    for article in article_chunk:\n",
    "        data.append({\n",
    "            \"title\": article.get(\"title\"),\n",
    "            \"description\": article.get(\"description\"),\n",
    "            \"text\": article.get(\"text\"),\n",
    "            \"keywords\": article.get(\"keywords\"),\n",
    "            \"author\": article.get(\"author\"),\n",
    "            \"date\": article.get(\"date\")\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Beispiel-Chunks verarbeiten\n",
    "df_articles = extract_fields(sample_chunk)\n",
    "\n",
    "# Test Datensatz erstellen\n",
    "test_df = df_articles.sample(40)\n",
    "\n",
    "test_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09218485",
   "metadata": {},
   "source": [
    "## 3. Sentiment-Analyse auf Artikelbeschreibungen\n",
    "Wir verwenden ein Transformer-Modell, das speziell für die deutsche Sprache trainiert wurde, um die Stimmung der Artikelbeschreibungen zu analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment-Analyse Pipeline mit einem deutschen Modell initialisieren\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"oliverguhr/german-sentiment-bert\")\n",
    "\n",
    "# Sentiment-Analyse auf die Beschreibungen anwenden\n",
    "df_articles['sentiment'] = df_articles['description'].apply(lambda x: sentiment_analyzer(x)[0]['label'] if pd.notnull(x) else None)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "df_articles[['title', 'description', 'sentiment']].head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e602c9",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition (NER) auf Artikeltexten\n",
    "Mit Hilfe eines multilingualen NER-Modells analysieren wir die wichtigsten Entitäten (z. B. Personen, Organisationen, Orte), die in den Artikeln genannt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda44c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NER-Pipeline mit einem multilingualen Modell für die deutsche Sprache\n",
    "ner_analyzer = pipeline(\"ner\", model=\"Davlan/xlm-roberta-base-ner-hrl\", grouped_entities=True)\n",
    "\n",
    "# NER auf den Artikeltext anwenden\n",
    "df_articles['entities'] = df_articles['text'].apply(lambda x: ner_analyzer(x) if pd.notnull(x) else None)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "df_articles[['title', 'entities']].head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335ca4a",
   "metadata": {},
   "source": [
    "## 5. Keyword-Extraktion aus dem Keyword-Feld\n",
    "Wir analysieren die häufig vorkommenden Schlagwörter, um wiederkehrende Themen zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfe0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Schlagwörter splitten und Häufigkeiten zählen\n",
    "all_keywords = df_articles['keywords'].dropna().str.split(', ').sum()\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# Umwandeln in DataFrame zur Visualisierung\n",
    "df_keywords = pd.DataFrame(keyword_counts.items(), columns=['Keyword', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "df_keywords.head(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d311e8",
   "metadata": {},
   "source": [
    "## 6. Topic Modeling auf Artikeltexten\n",
    "Mit Topic Modeling können wir zugrunde liegende Themen in den Artikeln identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653903dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Textdaten für Topic Modeling vorbereiten\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='german')\n",
    "text_matrix = vectorizer.fit_transform(df_articles['text'].dropna())\n",
    "\n",
    "# LDA-Analyse durchführen\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(text_matrix)\n",
    "\n",
    "# Top-Wörter für jedes Thema anzeigen\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"Thema #{index+1}:\")\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc85d15",
   "metadata": {},
   "source": [
    "## 7. Visualisierung der Sentiment- und Keyword-Analyse\n",
    "Abschließend visualisieren wir die Häufigkeit der wichtigsten Schlagwörter sowie die Verteilung der Stimmung in den Artikelbeschreibungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top-Schlagwörter plotten\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_keywords.head(10).plot(kind='barh', x='Keyword', y='Frequency', legend=False, color='skyblue')\n",
    "plt.title('Top-Schlagwörter in Artikeln')\n",
    "plt.xlabel('Häufigkeit')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment-Verteilung plotten\n",
    "sentiment_counts = df_articles['sentiment'].value_counts()\n",
    "plt.figure(figsize=(6, 6))\n",
    "sentiment_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Sentiment-Verteilung in Artikelbeschreibungen')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693d639",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "In diesem Notebook haben wir verschiedene NLP-Methoden auf einen großen Datensatz von deutschen Nachrichtenartikeln angewendet. Durch Sentiment-Analyse, Named Entity Recognition und Topic Modeling haben wir nützliche Erkenntnisse aus den Artikeldaten gewonnen. Diese Methoden können weiter genutzt werden, um tiefere Analysen in journalistischen Projekten durchzuführen.\n",
    "\n",
    "**Vielen Dank für die Teilnahme!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
