{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb0ea52",
   "metadata": {},
   "source": [
    "# Was ist und kann Transformers?\n",
    "Kurze Einführung und Vorstellung der Funktionen des transformers python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des Pakets (Nicht notwendig bei Google Colab)\n",
    "!pip install pandas transformers torchaudio pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c182a",
   "metadata": {},
   "source": [
    "## Transkription von Audiodateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd74f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Neue Pipeline erstellen und Aufgabe zuweisen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959afb88",
   "metadata": {},
   "source": [
    "## Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855636d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Datensätze laden und untersuchen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91d487",
   "metadata": {},
   "source": [
    "# Analyse von Nachrichtenartikeln\n",
    "In diesem Abschnitt verwenden wir verschiedene NLP-Tools von Hugging Face, um einen großen Datensatz voller Spiegel-Artikel zu analysieren. Mit Hilfe von Transformer-Modellen führen wir Sentiment-Analyse, Named Entity Recognition (NER) und Topic Modeling durch, um wichtige Einblicke in die Artikel zu gewinnen.\n",
    "\n",
    "Archiv unter: https://www.spiegel.de/spiegel/print/index-2002-1.html\n",
    "\n",
    "### Ziel:\n",
    "- Laden und Verarbeiten großer JSON-Datensätze in Chunks\n",
    "- Durchführung von Sentiment-Analyse und NER auf Artikeldaten\n",
    "- Visualisierung und Analyse der Ergebnisse\n",
    "\n",
    "**Benötigte Zeit:** ~1-2 Stunden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d576ce5",
   "metadata": {},
   "source": [
    "## 1. Setup und Laden der JSON-Daten in Chunks\n",
    "Wir beginnen mit der Installation der notwendigen Bibliotheken und laden die JSON-Daten in verwaltbaren Chunks, damit wir große Dateien effizient verarbeiten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# JSON-Daten in Chunks laden\n",
    "def load_json_from_github_in_chunks(url, chunk_size=1000):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    articles = list(data.values())\n",
    "    for i in range(0, len(articles), chunk_size):\n",
    "        yield articles[i:i + chunk_size]\n",
    "\n",
    "# Pfad zur Datei angeben (Pfad entsprechend in Google Colab anpassen oder direkt von URL ziehen)\n",
    "github_url = 'https://raw.githubusercontent.com/dajo51/hands-on-msc-journalismus/main/data_2009.json'\n",
    "\n",
    "# Beispiel-Chunks laden und Struktur inspizieren\n",
    "sample_chunk = next(load_json_from_github_in_chunks(github_url))\n",
    "df_sample = pd.DataFrame(sample_chunk)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5ebf",
   "metadata": {},
   "source": [
    "## 2. Extrahieren und Verarbeiten der Schlüsselinformationen\n",
    "Wir transformieren die JSON-Daten in ein DataFrame-Format, um sie leichter analysieren zu können. Hier extrahieren wir Felder wie `title`, `description`, `text` und `keywords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ea988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funktion zum Extrahieren der Schlüssel-Felder\n",
    "\n",
    "# Beispiel-Chunks verarbeiten\n",
    "\n",
    "# Test Datensatz erstellen\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09218485",
   "metadata": {},
   "source": [
    "## 3. Sentiment-Analyse auf Artikelbeschreibungen\n",
    "Wir verwenden ein Transformer-Modell, das speziell für die deutsche Sprache trainiert wurde, um die Stimmung der Artikelbeschreibungen zu analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment-Analyse Pipeline mit einem deutschen Modell initialisieren\n",
    "\n",
    "# Sentiment-Analyse auf die Beschreibungen anwenden\n",
    "\n",
    "# Ergebnisse anzeigen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e602c9",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition (NER) auf Artikeltexten\n",
    "Mit Hilfe eines multilingualen NER-Modells analysieren wir die wichtigsten Entitäten (z. B. Personen, Organisationen, Orte), die in den Artikeln genannt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda44c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NER-Pipeline mit einem multilingualen Modell für die deutsche Sprache\n",
    "\n",
    "# NER auf den Artikeltext anwenden\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier ist Platz für euren Visualisierungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335ca4a",
   "metadata": {},
   "source": [
    "## 5. Keyword-Extraktion aus dem Keyword-Feld\n",
    "Wir analysieren die häufig vorkommenden Schlagwörter, um wiederkehrende Themen zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfe0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Schlagwörter splitten und Häufigkeiten zählen\n",
    "\n",
    "\n",
    "# Umwandeln in DataFrame zur Visualisierung\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d311e8",
   "metadata": {},
   "source": [
    "## 6. Advanced: Topic Modeling auf Artikeltexten\n",
    "Mit Topic Modeling können wir zugrunde liegende Themen in den Artikeln identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653903dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Textdaten für Topic Modeling vorbereiten\n",
    "\n",
    "# LDA-Analyse durchführen\n",
    "\n",
    "# Top-Wörter für jedes Thema anzeigen\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc85d15",
   "metadata": {},
   "source": [
    "## 7. Visualisierung der Sentiment- und Keyword-Analyse\n",
    "Abschließend visualisieren wir die Häufigkeit der wichtigsten Schlagwörter sowie die Verteilung der Stimmung in den Artikelbeschreibungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top-Schlagwörter plotten\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_keywords.head(10).plot(kind='barh', x='Keyword', y='Frequency', legend=False, color='skyblue')\n",
    "plt.title('Top-Schlagwörter in Artikeln')\n",
    "plt.xlabel('Häufigkeit')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment-Verteilung plotten\n",
    "sentiment_counts = df_articles['sentiment'].value_counts()\n",
    "plt.figure(figsize=(6, 6))\n",
    "sentiment_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Sentiment-Verteilung in Artikelbeschreibungen')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693d639",
   "metadata": {},
   "source": [
    "## 8. Zusammenfassung\n",
    "In diesem Notebook haben wir verschiedene NLP-Methoden auf einen großen Datensatz von deutschen Nachrichtenartikeln angewendet. Durch Sentiment-Analyse, Named Entity Recognition und Topic Modeling haben wir nützliche Erkenntnisse aus den Artikeldaten gewonnen. Diese Methoden können weiter genutzt werden, um tiefere Analysen in journalistischen Projekten durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0e043",
   "metadata": {},
   "source": [
    "## 9. Gruppenübung\n",
    "Habt ihr Ideen zur Analyse, Auswertung und Visualisierung der Spiegel Daten oder eines eigenes Datensatzes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000434d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier starten"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
