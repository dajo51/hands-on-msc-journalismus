{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the news website (replace with a real URL if possible)\n",
    "url = \"https://example.com/news\"\n",
    "\n",
    "# Request and parse the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract and store headlines in a list\n",
    "headlines = [h2.text for h2 in soup.find_all('h2')]\n",
    "\n",
    "headlines = [\n",
    "    \"Government Announces New Climate Change Policy\",\n",
    "    \"Local Schools Embrace Remote Learning Technology\",\n",
    "    \"Tech Giants Report Record Profits Amid Economic Downturn\",\n",
    "    \"Scientists Discover New Species in the Amazon Rainforest\",\n",
    "    \"City Council Approves Plan to Revitalize Downtown Area\",\n",
    "    \"Healthcare Workers Demand Better Working Conditions\",\n",
    "    \"Breakthrough in Cancer Research Offers New Hope\",\n",
    "    \"Sports Team Wins Championship After Decade-Long Drought\",\n",
    "    \"Experts Warn of Rising Sea Levels Along Coastal Cities\",\n",
    "    \"Controversial Bill Passes in the Senate by Narrow Margin\",\n",
    "    \"Study Shows Link Between Diet and Mental Health\",\n",
    "    \"Housing Prices Continue to Soar in Major Cities\",\n",
    "    \"Startup Disrupts Industry with Innovative New Product\",\n",
    "    \"Wildfire Spreads Across Thousands of Acres\",\n",
    "    \"Local Artists Hold Exhibition on Climate Awareness\",\n",
    "    \"Unemployment Rates Drop as Economy Recovers\",\n",
    "    \"Cybersecurity Firm Reports Increase in Data Breaches\",\n",
    "    \"New Initiative Aims to Improve Literacy Rates\",\n",
    "    \"University Launches AI Research Center\",\n",
    "    \"Public Protests Against Government Surveillance Measures\"\n",
    "]\n",
    "\n",
    "print(\"Headlines:\", headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Analyze sentiment for each headline\n",
    "sentiment_results = []\n",
    "for headline in headlines:\n",
    "    result = sentiment_analyzer(headline)[0]\n",
    "    sentiment_results.append({\n",
    "        'Headline': headline,\n",
    "        'Sentiment': result['label'],\n",
    "        'Score': result['score']\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easy viewing and manipulation\n",
    "sentiment_df = pd.DataFrame(sentiment_results)\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of headlines in each sentiment category\n",
    "sentiment_counts = sentiment_df['Sentiment'].value_counts()\n",
    "\n",
    "# Plot sentiment distribution as a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values, color=['skyblue', 'salmon'])\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Headlines\")\n",
    "plt.title(\"Sentiment Distribution of News Headlines\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Tokenize and remove common stop words\n",
    "stop_words = {'the', 'is', 'on', 'and', 'a', 'of', 'to', 'in', 'for'}\n",
    "words = [word.lower() for headline in headlines for word in re.findall(r'\\w+', headline) if word.lower() not in stop_words]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "common_words = word_counts.most_common(10)\n",
    "print(\"Top 10 Common Words:\", common_words)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "words_df = pd.DataFrame(common_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# Plot common words as a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words_df['Word'], words_df['Frequency'], color='lightgreen')\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 10 Common Words in Headlines\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conclusion\n",
    "In this notebook, we explored:\n",
    "- How to scrape news headlines using Python and BeautifulSoup.\n",
    "- How to use a pre-trained AI model to perform sentiment analysis on news headlines.\n",
    "- How to visualize trends and patterns in sentiment and keyword frequency.\n",
    "\n",
    "These techniques provide powerful ways for journalists to analyze news coverage and public sentiment. \n",
    "We hope you feel inspired to continue exploring Python and AI for data journalism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load JSON data in chunks\n",
    "def load_json_in_chunks(file_path, chunk_size=1000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    articles = list(data.values())\n",
    "    for i in range(0, len(articles), chunk_size):\n",
    "        yield articles[i:i + chunk_size]\n",
    "\n",
    "# Define file path (replace with your file path in Colab)\n",
    "file_path = 'data_2009.json'\n",
    "\n",
    "# Load a sample chunk to inspect structure\n",
    "sample_chunk = next(load_json_in_chunks(file_path))\n",
    "df_sample = pd.DataFrame(sample_chunk)\n",
    "df_sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract key fields into a DataFrame\n",
    "def extract_fields(article_chunk):\n",
    "    data = []\n",
    "    for article in article_chunk:\n",
    "        data.append({\n",
    "            \"title\": article.get(\"title\"),\n",
    "            \"description\": article.get(\"description\"),\n",
    "            \"text\": article.get(\"text\"),\n",
    "            \"keywords\": article.get(\"keywords\"),\n",
    "            \"author\": article.get(\"author\"),\n",
    "            \"date\": article.get(\"date\")\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Process sample chunk\n",
    "df_articles = extract_fields(sample_chunk)\n",
    "df_articles.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentiment analysis pipeline with a German model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"oliverguhr/german-sentiment-bert\")\n",
    "\n",
    "# Apply sentiment analysis to each description\n",
    "df_articles['sentiment'] = df_articles['description'].apply(lambda x: sentiment_analyzer(x)[0]['label'] if pd.notnull(x) else None)\n",
    "\n",
    "# View sentiment analysis results\n",
    "df_articles[['title', 'description', 'sentiment']].head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
