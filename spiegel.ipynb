{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f91d487",
   "metadata": {},
   "source": [
    "# Analyse von Nachrichtenartikeln in deutscher Sprache\n",
    "In diesem Notebook verwenden wir verschiedene NLP-Tools von Hugging Face, um große Datensätze von Nachrichtenartikeln in deutscher Sprache zu analysieren. Mit Hilfe von Transformer-Modellen führen wir Sentiment-Analyse, Named Entity Recognition (NER) und Topic Modeling durch, um wichtige Einblicke in die Artikel zu gewinnen.\n",
    "\n",
    "### Ziel:\n",
    "- Laden und Verarbeiten großer JSON-Datensätze in Chunks\n",
    "- Durchführung von Sentiment-Analyse und NER auf Artikeldaten\n",
    "- Visualisierung und Analyse der Ergebnisse\n",
    "\n",
    "**Benötigte Zeit:** ~2 Stunden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d576ce5",
   "metadata": {},
   "source": [
    "## 1. Setup und Laden der JSON-Daten in Chunks\n",
    "Wir beginnen mit der Installation der notwendigen Bibliotheken und laden die JSON-Daten in verwaltbaren Chunks, damit wir große Dateien effizient verarbeiten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bibliotheken importieren\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# JSON-Daten in Chunks laden\n",
    "def load_json_from_github_in_chunks(url, chunk_size=1000):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    articles = list(data.values())\n",
    "    for i in range(0, len(articles), chunk_size):\n",
    "        yield articles[i:i + chunk_size]\n",
    "\n",
    "# Pfad zur Datei angeben (Pfad entsprechend in Google Colab anpassen)\n",
    "github_url = 'https://raw.githubusercontent.com/dajo51/hands-on-msc-journalismus/main/data_2009.json'\n",
    "\n",
    "# Beispiel-Chunks laden und Struktur inspizieren\n",
    "sample_chunk = next(load_json_from_github_in_chunks(github_url))\n",
    "df_sample = pd.DataFrame(sample_chunk)\n",
    "df_sample.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5ebf",
   "metadata": {},
   "source": [
    "## 2. Extrahieren und Verarbeiten der Schlüsselinformationen\n",
    "Wir transformieren die JSON-Daten in ein DataFrame-Format, um sie leichter analysieren zu können. Hier extrahieren wir Felder wie `title`, `description`, `text` und `keywords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ea988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funktion zum Extrahieren der Schlüssel-Felder\n",
    "def extract_fields(article_chunk):\n",
    "    data = []\n",
    "    for article in article_chunk:\n",
    "        data.append({\n",
    "            \"title\": article.get(\"title\"),\n",
    "            \"description\": article.get(\"description\"),\n",
    "            \"text\": article.get(\"text\"),\n",
    "            \"keywords\": article.get(\"keywords\"),\n",
    "            \"author\": article.get(\"author\"),\n",
    "            \"date\": article.get(\"date\")\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Beispiel-Chunks verarbeiten\n",
    "df_articles = extract_fields(sample_chunk)\n",
    "df_articles.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09218485",
   "metadata": {},
   "source": [
    "## 3. Sentiment-Analyse auf Artikelbeschreibungen\n",
    "Wir verwenden ein Transformer-Modell, das speziell für die deutsche Sprache trainiert wurde, um die Stimmung der Artikelbeschreibungen zu analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment-Analyse Pipeline mit einem deutschen Modell initialisieren\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"oliverguhr/german-sentiment-bert\")\n",
    "\n",
    "# Sentiment-Analyse auf die Beschreibungen anwenden\n",
    "df_articles['sentiment'] = df_articles['description'].apply(lambda x: sentiment_analyzer(x)[0]['label'] if pd.notnull(x) else None)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "df_articles[['title', 'description', 'sentiment']].head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e602c9",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition (NER) auf Artikeltexten\n",
    "Mit Hilfe eines multilingualen NER-Modells analysieren wir die wichtigsten Entitäten (z. B. Personen, Organisationen, Orte), die in den Artikeln genannt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda44c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NER-Pipeline mit einem multilingualen Modell für die deutsche Sprache\n",
    "ner_analyzer = pipeline(\"ner\", model=\"Davlan/xlm-roberta-base-ner-hrl\", grouped_entities=True)\n",
    "\n",
    "# NER auf den Artikeltext anwenden\n",
    "df_articles['entities'] = df_articles['text'].apply(lambda x: ner_analyzer(x) if pd.notnull(x) else None)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "df_articles[['title', 'entities']].head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335ca4a",
   "metadata": {},
   "source": [
    "## 5. Keyword-Extraktion aus dem Keyword-Feld\n",
    "Wir analysieren die häufig vorkommenden Schlagwörter, um wiederkehrende Themen zu identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfe0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Schlagwörter splitten und Häufigkeiten zählen\n",
    "all_keywords = df_articles['keywords'].dropna().str.split(', ').sum()\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# Umwandeln in DataFrame zur Visualisierung\n",
    "df_keywords = pd.DataFrame(keyword_counts.items(), columns=['Keyword', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "df_keywords.head(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d311e8",
   "metadata": {},
   "source": [
    "## 6. Topic Modeling auf Artikeltexten\n",
    "Mit Topic Modeling können wir zugrunde liegende Themen in den Artikeln identifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653903dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Textdaten für Topic Modeling vorbereiten\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='german')\n",
    "text_matrix = vectorizer.fit_transform(df_articles['text'].dropna())\n",
    "\n",
    "# LDA-Analyse durchführen\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(text_matrix)\n",
    "\n",
    "# Top-Wörter für jedes Thema anzeigen\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"Thema #{index+1}:\")\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc85d15",
   "metadata": {},
   "source": [
    "## 7. Visualisierung der Sentiment- und Keyword-Analyse\n",
    "Abschließend visualisieren wir die Häufigkeit der wichtigsten Schlagwörter sowie die Verteilung der Stimmung in den Artikelbeschreibungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Top-Schlagwörter plotten\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_keywords.head(10).plot(kind='barh', x='Keyword', y='Frequency', legend=False, color='skyblue')\n",
    "plt.title('Top-Schlagwörter in Artikeln')\n",
    "plt.xlabel('Häufigkeit')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment-Verteilung plotten\n",
    "sentiment_counts = df_articles['sentiment'].value_counts()\n",
    "plt.figure(figsize=(6, 6))\n",
    "sentiment_counts.plot(kind='pie', autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Sentiment-Verteilung in Artikelbeschreibungen')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693d639",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "In diesem Notebook haben wir verschiedene NLP-Methoden auf einen großen Datensatz von deutschen Nachrichtenartikeln angewendet. Durch Sentiment-Analyse, Named Entity Recognition und Topic Modeling haben wir nützliche Erkenntnisse aus den Artikeldaten gewonnen. Diese Methoden können weiter genutzt werden, um tiefere Analysen in journalistischen Projekten durchzuführen.\n",
    "\n",
    "**Vielen Dank für die Teilnahme!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
