{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954f181f",
   "metadata": {},
   "source": [
    "# Public Data Analysis for Journalism\n",
    "This notebook demonstrates the workflow for collecting public data, analyzing it with AI tools (like Hugging Face's transformers), processing it with pandas, and visualizing insights. \n",
    "\n",
    "### Goals:\n",
    "- Learn how to scrape data\n",
    "- Analyze sentiment and detect key entities\n",
    "- Visualize insights for journalistic storytelling\n",
    "\n",
    "**Estimated time:** ~2.5 hours, with group tasks for deeper exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052039b8",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests beautifulsoup4 pandas transformers matplotlib openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882df7d1",
   "metadata": {},
   "source": [
    "## 2. Data Collection with Web Scraping\n",
    "Let's start by scraping some sample headlines from a website. This will help us collect public data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6275ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the news website (replace with a real URL if possible)\n",
    "url = \"https://example.com/news\"\n",
    "\n",
    "# Request and parse the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract and store headlines in a list\n",
    "headlines = [h2.text for h2 in soup.find_all('h2')]\n",
    "print(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e2037",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing with Pandas\n",
    "Now that we have our headlines, let's clean the data to prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load headlines into a DataFrame\n",
    "df = pd.DataFrame(headlines, columns=[\"Headline\"])\n",
    "\n",
    "# Sample cleaning - remove punctuation, lowercase\n",
    "df[\"Cleaned_Headline\"] = df[\"Headline\"].str.replace(r'[^\\w\\s]', '', regex=True).str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe3868",
   "metadata": {},
   "source": [
    "## 4. Text Analysis with Transformers\n",
    "### 4.1 Sentiment Analysis\n",
    "We'll use a pre-trained model to analyze the sentiment of each headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611526fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Apply sentiment analysis to each headline\n",
    "df[\"Sentiment\"] = df[\"Headline\"].apply(lambda x: sentiment_analyzer(x)[0]['label'])\n",
    "df[\"Score\"] = df[\"Headline\"].apply(lambda x: sentiment_analyzer(x)[0]['score'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de88f8f",
   "metadata": {},
   "source": [
    "### 4.2 Named Entity Recognition\n",
    "We'll identify key entities (people, places, organizations) in each headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d311eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_analyzer = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "# Extract entities for each headline\n",
    "df[\"Entities\"] = df[\"Headline\"].apply(lambda x: ner_analyzer(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb47778a",
   "metadata": {},
   "source": [
    "## 5. Data Visualization\n",
    "### 5.1 Sentiment Distribution\n",
    "Let's visualize the distribution of sentiment across our collected headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ab643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_counts = df[\"Sentiment\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values, color=['skyblue', 'salmon'])\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Sentiment Distribution of Headlines\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfe470",
   "metadata": {},
   "source": [
    "### 5.2 Entity Frequency Visualization\n",
    "Now, let's visualize the most common entities found in our headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0cf1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten entity lists and count occurrences\n",
    "entities = [entity['word'] for sublist in df[\"Entities\"] for entity in sublist]\n",
    "entity_counts = Counter(entities).most_common(10)\n",
    "\n",
    "# Plot the most common entities\n",
    "entity_names, entity_values = zip(*entity_counts)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(entity_names, entity_values, color='lightgreen')\n",
    "plt.xlabel(\"Entity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 10 Most Common Entities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08884143",
   "metadata": {},
   "source": [
    "## 6. Exporting Data to Excel\n",
    "Finally, let's export our data to an Excel file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"news_headlines_analysis.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9a259",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up and Discussion\n",
    "In this notebook, we covered:\n",
    "- Collecting public data with web scraping\n",
    "- Cleaning and analyzing the data using pandas and transformers\n",
    "- Visualizing insights to tell stories with data\n",
    "\n",
    "**Take-Home Challenge**: Apply these techniques to another dataset, such as social media data, to analyze sentiment on a specific topic.\n",
    "\n",
    "### Thank you for participating! Feel free to ask questions and explore more on your own."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
